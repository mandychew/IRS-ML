{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MU7k0WUN7dPF"
      },
      "outputs": [],
      "source": [
        "#what\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeGnxU-OYpwV",
        "outputId": "408d93ff-1c26-4e5d-e278-65743fa69524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USrS6My477ym"
      },
      "outputs": [],
      "source": [
        "#xdf = pd.read_csv('/content/drive/MyDrive/ML/Datasets/Chinese_segmentation/as_training.utf8', header=None, names=['raw_words'])\n",
        "#xfor_vocab = open('/content/drive/MyDrive/ML/Datasets/Chinese_segmentation/as_training.utf8')\n",
        "\n",
        "#testdf = pd.read_csv('/content/drive/MyDrive/ML/Datasets/Chinese_segmentation/as_test.utf8', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WNxc9hWFYgi4"
      },
      "outputs": [],
      "source": [
        "xdf = pd.read_csv(r'as_training.utf8', header=None, names=['raw_words'])\n",
        "\n",
        "#personal path just ignore\n",
        "\n",
        "#xdf = pd.read_csv(r'C:\\Users\\zhiha\\OneDrive\\Documents\\.Zhihao\\.ML\\datasets\\Chinese Segmentation\\as_training.utf8', header=None, names=['raw_words'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mie5F4TgCjGk"
      },
      "outputs": [],
      "source": [
        "def spaceornot(line):\n",
        "  splace = []\n",
        "  listed = list(line)\n",
        "  for i, char in enumerate(listed):\n",
        "    if listed[i] == listed[-1]:\n",
        "      splace.append(1)\n",
        "    elif char == '\\u3000':\n",
        "      pass\n",
        "    elif listed[i + 1] == '\\u3000':\n",
        "      splace.append(1)\n",
        "    elif listed[i + 1] != '\\u3000':\n",
        "      splace.append(0)\n",
        "  return splace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M1RHsgw8-heC",
        "outputId": "5f7efdf6-eac9-4673-cd2b-04d0b86636d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nused data:\\ntrain_data: xseq\\ntrain_label: yactual\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yraw = xdf.copy()\n",
        "\n",
        "xtrain = pd.DataFrame(xdf['raw_words'].str.replace('\\u3000',''))   #could ignore last punctuation\n",
        "\n",
        "#tokening _input data\n",
        "tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
        "        #dlist_sent = seriestolist(xtrain, 0)\n",
        "tokenizer.fit_on_texts(xtrain['raw_words'])\n",
        "word_index = tokenizer.word_index\n",
        "xtrain['sequenced_sent'] = tokenizer.texts_to_sequences(xtrain['raw_words'])\n",
        "X = pad_sequences(xtrain['sequenced_sent'], padding='post')\n",
        "\n",
        "#y_actual\n",
        "yraw['labels'] = yraw['raw_words'].map(spaceornot)\n",
        "Y = pad_sequences(yraw['labels'], padding='post')\n",
        "\n",
        "\n",
        "# alternatively, you can pass the padded arrays into the dataframe by converting it into a list using list()\n",
        "\"\"\"\n",
        "used data:\n",
        "train_data: xseq\n",
        "train_label: yactual\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VNm4SbcZ3Jgu"
      },
      "outputs": [],
      "source": [
        "val_split = 0.75\n",
        "vsize = len(word_index)\n",
        "sentlen = len(X[0,:])\n",
        "em_dim = 256\n",
        "epol = 1000\n",
        "basize = 1024\n",
        "\n",
        "#callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aky4VCs3Ks0M"
      },
      "outputs": [],
      "source": [
        "#splitting dataset\n",
        "spli = int(val_split * len(X))\n",
        "trainX, testX = X[:spli], X[spli:]\n",
        "trainY, testY = Y[:spli], Y[spli:]\n",
        "\n",
        "\n",
        "#possible to split in ~model~.fit() function too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ngt7nEm7h22X"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Custom Loss\n",
        "'''\n",
        "\n",
        "class MaskedSequenceLoss(tf.keras.losses.Loss):\n",
        "    def __init__(\n",
        "        self,\n",
        "        average_across_timesteps=False,\n",
        "        average_across_batch=False,\n",
        "        sum_over_timesteps=True,\n",
        "        sum_over_batch=True,\n",
        "        softmax_loss_function=None,\n",
        "        name=None,\n",
        "        reduction=None, # dummy arg so it can be used as custom object when loading saved model\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.opts = {\n",
        "            \"average_across_timesteps\": average_across_timesteps,\n",
        "            \"average_across_batch\": average_across_batch,\n",
        "            \"sum_over_timesteps\": sum_over_timesteps,\n",
        "            \"sum_over_batch\": sum_over_batch,\n",
        "            \"softmax_loss_function\": softmax_loss_function,\n",
        "            \"name\": name,\n",
        "        }\n",
        "    \n",
        "    def call(self, y_true, y_pred):\n",
        "        return tfa.seq2seq.sequence_loss(y_pred, y_true,\n",
        "                                         weights=tf.cast(y_pred._keras_mask, tf.float32) if hasattr(y_pred, \"_keras_mask\") else tf.ones(y_true.shape),\n",
        "                                         **self.opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL1QUTngMliB",
        "outputId": "c73a51eb-7b56-4330-9cc8-eee55cf7b57c"
      },
      "outputs": [],
      "source": [
        "xin = Input((sentlen,))\n",
        "\n",
        "x = Embedding(vsize, em_dim, input_length=sentlen, mask_zero = True)(xin) #ignore padded zeros -> mask_zero = True\n",
        "x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "\n",
        "x = Dense(128, activation='swish')(x)\n",
        "x = Dropout(0.7)(x)\n",
        "x = Dense(64, activation='swish')(x) \n",
        "x = Dropout(0.7)(x)\n",
        "x = Dense(64, activation='swish')(x) \n",
        "x = Dropout(0.7)(x)\n",
        "\n",
        "xout = Dense(2, activation='linear')(x)\n",
        "\n",
        "segcl = Model(xin, xout)\n",
        "segcl.compile(optimizer='adam', loss=MaskedSequenceLoss(), metrics=['accuracy'])\n",
        "segcl.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dibiJmHx3rX3"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.1, verbose=1),\n",
        "    tf.keras.callbacks.ModelCheckpoint('./best_models', monitor='val_acc', save_best_only=True)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQAuG_pJP1Hg",
        "outputId": "b89fffb0-5691-4194-9af2-dc7fa080bba4"
      },
      "outputs": [],
      "source": [
        "segcl.fit(\n",
        "    trainX, \n",
        "    trainY,\n",
        "    validation_data=(testX, testY),\n",
        "    epochs = epol,\n",
        "    batch_size = basize,\n",
        "    callbacks = callbacks\n",
        ")\n",
        "\n",
        "#you can use validation_split = True to split the data into val and train, instead of manual splitting\n",
        "#using tf.stack, you can pass the dataframe series into it directly\n",
        "'''\n",
        "segcl.fit(\n",
        "\n",
        ")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FGOEFsCHYgi-"
      },
      "outputs": [],
      "source": [
        "def splitter(sentences, text_split_indexes):\n",
        "    dat = pd.DataFrame(sentences, columns=['unsplit'])\n",
        "    splited = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        split_sen = ''\n",
        "        for n, sply in enumerate(list(text_split_indexes[i])[:len(sentence)]):\n",
        "            split_sen += sentence[n]\n",
        "            if sply == 1:\n",
        "                split_sen += '\\u3000'\n",
        "\n",
        "        splited.append(split_sen)\n",
        "    dat['splited'] = pd.DataFrame(splited)\n",
        "\n",
        "    return dat\n",
        "\n",
        "def display(dataframe):\n",
        "    for i in range(len(dataframe.iloc[:,0])):\n",
        "        print('original sentence:  ' + dataframe.iloc[i, 0])\n",
        "        print('splitted sentence:  ' + dataframe.iloc[i, 1])\n",
        "        print('\\n')\n",
        "\n",
        "\n",
        "def display_with_correct(dataframe, correct):\n",
        "    for i in range(len(dataframe.iloc[:,0])):\n",
        "        print('original sentence:  ' + dataframe.iloc[i, 0])\n",
        "        print('splitted sentence:  ' + dataframe.iloc[i, 1])\n",
        "        print('actually sentence:  ' + correct.iloc[i, 0])\n",
        "        print('\\n')\n",
        "\n",
        "def entire(input):\n",
        "    input_list = []\n",
        "    input_list.append(input)\n",
        "    token = tokenizer.texts_to_sequences(input_list)\n",
        "    token = pad_sequences(token, padding='post', maxlen=sentlen)\n",
        "\n",
        "    pred = segcl.predict(token)\n",
        "    pred = tf.nn.softmax(pred)\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "    return display(splitter(input_list, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jul9jOdIYgi-"
      },
      "outputs": [],
      "source": [
        "input_sentences=[\n",
        "    '生日快樂',\n",
        "    '我的名字是',\n",
        "    '今天的天氣很不可思議',\n",
        "    '中文單詞',\n",
        "    '雪花飄飄北風蕭蕭',\n",
        "    'Google 的免費服務可即時在英語和 100 多種其他語言之間翻譯單詞、短語和網頁。'\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocmcuySBYgi-"
      },
      "outputs": [],
      "source": [
        "input_sentences = list(xtrain.iloc[:10,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzVTGrYPYxOd"
      },
      "outputs": [],
      "source": [
        "token_sentences = tokenizer.texts_to_sequences(input_sentences)\n",
        "token_sentences = pad_sequences(token_sentences, padding='post', maxlen=sentlen)\n",
        "\n",
        "text_predictions = segcl.predict(token_sentences)\n",
        "text_predictions = tf.nn.softmax(text_predictions)\n",
        "text_predictions = np.argmax(text_predictions, axis=-1)\n",
        "\n",
        "org_data = splitter(input_sentences, text_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-7vD5bKYgi_",
        "outputId": "b2a2f020-6548-4de9-86c8-5f727cf26119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original sentence:  生日快樂\n",
            "splitted sentence:  生日　快樂　\n",
            "\n",
            "\n",
            "original sentence:  我的名字是\n",
            "splitted sentence:  我　的　名字　是　\n",
            "\n",
            "\n",
            "original sentence:  今天的天氣很不可思議\n",
            "splitted sentence:  今天　的　天氣　很　不可思議　\n",
            "\n",
            "\n",
            "original sentence:  中文單詞\n",
            "splitted sentence:  中文　單詞　\n",
            "\n",
            "\n",
            "original sentence:  雪花飄飄北風蕭蕭\n",
            "splitted sentence:  雪花　飄飄　北　風　蕭蕭　\n",
            "\n",
            "\n",
            "original sentence:  Google 的免費服務可即時在英語和 100 多種其他語言之間翻譯單詞、短語和網頁。\n",
            "splitted sentence:  Googl　e 　的　免費　服務　可　即時　在　英語　和　 100 　多　種　其他　語言　之間　翻譯　單詞　、　短語　和　網頁　。　\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "display(org_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awjTDdlC-Slo",
        "outputId": "dbe38902-6bab-4b4d-c9a4-aff6aabc30ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original sentence:  雪花飄飄北風蕭蕭\n",
            "splitted sentence:  雪花　飄飄　北　風　蕭蕭　\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "entire('雪花飄飄北風蕭蕭')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "#done"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Chinese_segmentation_complete.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "38c7ce74dd526bc9e84fd0682f6c1ac8fcd6c4cb0e87d36fcf4e0e214217cc07"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
