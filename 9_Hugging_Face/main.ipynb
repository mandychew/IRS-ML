{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Crash Course\n",
    "Pipelines, Pretrained Model, Fine Tuning  \n",
    "Inspired by this [YouTube tutorial](https://youtu.be/GSt00_-0ncQ) (using pytorch).  \n",
    "Prerequisite:  `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification # for pytorch, same model but remove 'TF' in front\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)  \n",
    "An easy way to use models for inference, e.g. sentiment analysis, image classification, object detection, question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# feeding a sample text through pipeline\n",
    "result = classifier('We are very happy to show you the HuggingFace Transformers Library.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding a list of text through pipeline\n",
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "results = classifier(X_train)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a specific model in pipeline\n",
    "[AutoModels documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)  \n",
    "- for pytorch, `import AutoModelForSequenceClassification from transformers`\n",
    "- for tensorflow, `import TFAutoModelForSequenceClassification from transformers`  \n",
    "\n",
    "[AutoTokenizer documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier1 = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model and Tokenizer without pipeline\n",
    "Note: you'll get same results with or without pipeline!  \n",
    "Recommended to use pipeline unless you want to finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english' # 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "token_ids = tokenizer(\"We are very happy to show you the HuggingFace Transformers Library.\")\n",
    "print(token_ids)\n",
    "print('In input_ids, tokens 101 and 102 are the beginning and end of string tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [PreTrainedTokenizer documentation](https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) for parameter details.  \n",
    "Specifically for `return_tensors` parameter: click [here](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.return_tensors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()] # convert from label_id to class name\n",
    "\n",
    "print(outputs)\n",
    "print(predictions)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Reloading Saved Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and tokenizer (esp. after finetuning)\n",
    "save_directory = 'saved'\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Model Hub](https://huggingface.co/models)\n",
    "Use any model uploaded by community  \n",
    "To use any model, copy the name in your chosen model (e.g. `oliverguhr/german-sentiment-bert`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'oliverguhr/german-sentiment-bert'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train_german = [\"Mit keinem guten Ergebnis\", \"Das war unfair\", # negative\n",
    "            \"nicht so schlecht wie erwartet\", \"Das war gut!\", # positive\n",
    "            \"Sie fahrt ein grunes Auto\"] # neutral\n",
    "\n",
    "batch = tokenizer(X_train_german, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()]\n",
    "\n",
    "print(labels)\n",
    "print('Correct answer should be:')\n",
    "print('[negative, negative, positive, positive, neutral]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Implement the model J2s used for TIL NLP\n",
    "[`harshit345/xlsr-wav2vec-speech-emotion-recognition`](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirement packages\n",
    "!pip install git+https://github.com/huggingface/datasets.git\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install torchaudio\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name_or_path = \"harshit345/xlsr-wav2vec-speech-emotion-recognition\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for a sample\n",
    "path = ''   \n",
    "outputs = predict(path, sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6425117d13823fa8044a2c07c859b613f2362d94b282c9a4162ba20339fd2c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
