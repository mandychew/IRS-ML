{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Crash Course\n",
    "Pipelines, Pretrained Model, Fine Tuning  \n",
    "Inspired by this [YouTube tutorial](https://youtu.be/GSt00_-0ncQ) (using pytorch).  \n",
    "Prerequisite:  `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification # for pytorch, same model but remove 'TF' in front\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)  \n",
    "An easy way to use models for inference, e.g. sentiment analysis, image classification, object detection, question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997598528862}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# feeding a sample text through pipeline\n",
    "result = classifier('We are very happy to show you the HuggingFace Transformers Library.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9997598528862}\n",
      "{'label': 'NEGATIVE', 'score': 0.5308609008789062}\n"
     ]
    }
   ],
   "source": [
    "# feeding a list of text through pipeline\n",
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "results = classifier(X_train)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a specific model in pipeline\n",
    "[AutoModels documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)  \n",
    "- for pytorch, `import AutoModelForSequenceClassification from transformers`\n",
    "- for tensorflow, `import TFAutoModelForSequenceClassification from transformers`  \n",
    "\n",
    "[AutoTokenizer documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier1 = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model and Tokenizer without pipeline\n",
    "Note: you'll get same results with or without pipeline!  \n",
    "Recommended to use pipeline unless you want to finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [00:07<00:00, 37.4MB/s] \n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_57']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 17662, 12172, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "In input_ids, tokens 101 and 102 are the beginning and end of string tokens\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english' # 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "token_ids = tokenizer(\"We are very happy to show you the HuggingFace Transformers Library.\")\n",
    "print(token_ids)\n",
    "print('In input_ids, tokens 101 and 102 are the beginning and end of string tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [PreTrainedTokenizer documentation](https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) for parameter details.  \n",
    "Specifically for `return_tensors` parameter: click [here](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.return_tensors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
      "array([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996,\n",
      "        17662, 12172, 19081,  3075,  1012,   102],\n",
      "       [  101,  2057,  3246,  2017,  2123,  1005,  1056,  5223,  2009,\n",
      "         1012,   102,     0,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[-4.058453  ,  4.2757363 ],\n",
      "       [ 0.07809059, -0.03831829]], dtype=float32)>, hidden_states=None, attentions=None)\n",
      "tf.Tensor(\n",
      "[[2.4010611e-04 9.9975985e-01]\n",
      " [5.2906942e-01 4.7093061e-01]], shape=(2, 2), dtype=float32)\n",
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()] # convert from label_id to class name\n",
    "\n",
    "print(outputs)\n",
    "print(predictions)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Reloading Saved Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and tokenizer (esp. after finetuning)\n",
    "save_directory = 'saved'\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Model Hub](https://huggingface.co/models)\n",
    "Use any model uploaded by community  \n",
    "To use any model, copy the name in your chosen model (e.g. `oliverguhr/german-sentiment-bert`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 161/161 [00:00<00:00, 79.8kB/s]\n",
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 665kB/s]\n",
      "Downloading: 100%|██████████| 249k/249k [00:01<00:00, 239kB/s]  \n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 111kB/s]\n",
      "Downloading: 100%|██████████| 416M/416M [00:28<00:00, 15.4MB/s]    \n",
      "Some layers from the model checkpoint at oliverguhr/german-sentiment-bert were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at oliverguhr/german-sentiment-bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'negative', 'positive', 'positive', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "model_name = 'oliverguhr/german-sentiment-bert'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train_german = [\"Mit keinem guten Ergebnis\", \"Das war unfair\", # negative\n",
    "            \"nicht so schlecht wie erwartet\", \"Das war gut!\", # positive\n",
    "            \"Sie fahrt ein grunes Auto\"] # neutral\n",
    "\n",
    "batch = tokenizer(X_train_german, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()]\n",
    "\n",
    "print(labels)\n",
    "print('Correct answer should be:')\n",
    "print('[negative, negative, positive, positive, neutral]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Implement the model J2s used for TIL NLP\n",
    "`harshit345/xlsr-wav2vec-speech-emotion-recognition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    " from datasets import load_dataset\n",
    " import torch\n",
    " \n",
    " # load model and tokenizer\n",
    " processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    " model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "     \n",
    " # load dummy dataset and read soundfiles\n",
    " ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    " \n",
    " # tokenize\n",
    " input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    " \n",
    " # retrieve logits\n",
    " logits = model(input_values).logits\n",
    " \n",
    " # take argmax and decode\n",
    " predicted_ids = torch.argmax(logits, dim=-1)\n",
    " transcription = processor.batch_decode(predicted_ids)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6425117d13823fa8044a2c07c859b613f2362d94b282c9a4162ba20339fd2c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
