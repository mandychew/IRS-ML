{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Crash Course\n",
    "Pipelines, Pretrained Model, Fine Tuning  \n",
    "Inspired by this [YouTube tutorial](https://youtu.be/GSt00_-0ncQ) (using pytorch).  \n",
    "Prerequisite:  `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification # for pytorch, same model but remove 'TF' in front\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)  \n",
    "An easy way to use models for inference, e.g. sentiment analysis, image classification, object detection, question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# feeding a sample text through pipeline\n",
    "result = classifier('We are very happy to show you the HuggingFace Transformers Library.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding a list of text through pipeline\n",
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "results = classifier(X_train)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a specific model in pipeline\n",
    "[AutoModels documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)  \n",
    "- for pytorch, `import AutoModelForSequenceClassification from transformers`\n",
    "- for tensorflow, `import TFAutoModelForSequenceClassification from transformers`  \n",
    "\n",
    "[AutoTokenizer documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier1 = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model and Tokenizer without pipeline\n",
    "Note: you'll get same results with or without pipeline!  \n",
    "Recommended to use pipeline unless you want to finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english' # 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "token_ids = tokenizer(\"We are very happy to show you the HuggingFace Transformers Library.\")\n",
    "print(token_ids)\n",
    "print('In input_ids, tokens 101 and 102 are the beginning and end of string tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [PreTrainedTokenizer documentation](https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) for parameter details.  \n",
    "Specifically for `return_tensors` parameter: click [here](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.return_tensors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()] # convert from label_id to class name\n",
    "\n",
    "print(outputs)\n",
    "print(predictions)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Reloading Saved Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and tokenizer (esp. after finetuning)\n",
    "save_directory = 'saved'\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Model Hub](https://huggingface.co/models)\n",
    "Use any model uploaded by community  \n",
    "To use any model, copy the name in your chosen model (e.g. `oliverguhr/german-sentiment-bert`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'oliverguhr/german-sentiment-bert'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train_german = [\"Mit keinem guten Ergebnis\", \"Das war unfair\", # negative\n",
    "            \"nicht so schlecht wie erwartet\", \"Das war gut!\", # positive\n",
    "            \"Sie fahrt ein grunes Auto\"] # neutral\n",
    "\n",
    "batch = tokenizer(X_train_german, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()]\n",
    "\n",
    "print(labels)\n",
    "print('Correct answer should be:')\n",
    "print('[negative, negative, positive, positive, neutral]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pipelines with non-default models (i.e. other models in Model Hub)\n",
    "- if the model in question fits the default pipelines, see [this](https://huggingface.co/docs/transformersquicktour#use-another-model-and-tokenizer-in-the-pipeline) for more details\n",
    "- if that model does not fit into any default pipeline, you'll need to use it without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'oliverguhr/german-sentiment-bert'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train_german = [\"Mit keinem guten Ergebnis\", \"Das war unfair\", # negative\n",
    "            \"nicht so schlecht wie erwartet\", \"Das war gut!\", # positive\n",
    "            \"Sie fahrt ein grunes Auto\"] # neutral\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "result = classifier(X_train_german)\n",
    "print(result)\n",
    "print('Correct answer should be:')\n",
    "print('negative, negative, positive, positive, neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "If you can’t find a model for your use-case, you will need to fine-tune a pretrained model on your data. See the [fine-tuning tutorial](https://huggingface.co/docs/transformers/training) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Implement the model J2s used for TIL NLP\n",
    "[`harshit345/xlsr-wav2vec-speech-emotion-recognition`](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)  \n",
    "Note: this model uses pytorch, so import the right packages from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # requirement packages\n",
    "# !pip install git+https://github.com/huggingface/datasets.git\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install torchaudio\n",
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mandy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Downloading: 100%|██████████| 1.18G/1.18G [01:20<00:00, 15.8MB/s]  \n",
      "Some weights of the model checkpoint at harshit345/xlsr-wav2vec-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at harshit345/xlsr-wav2vec-speech-emotion-recognition and are newly initialized: ['projector.bias', 'projector.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"harshit345/xlsr-wav2vec-speech-emotion-recognition\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for a sample\n",
    "path = 'test.wav'   \n",
    "outputs = predict(path, sampling_rate)\n",
    "print(outputs) # correct label is angry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6425117d13823fa8044a2c07c859b613f2362d94b282c9a4162ba20339fd2c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
