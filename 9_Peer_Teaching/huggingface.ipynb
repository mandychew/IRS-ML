{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Crash Course\n",
    "Pipelines, Pretrained Model, Fine Tuning  \n",
    "Inspired by this [YouTube tutorial](https://youtu.be/GSt00_-0ncQ) (using pytorch).  \n",
    "Prerequisite:  `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification # for pytorch, same model but remove 'TF' in front\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)  \n",
    "An easy way to use models for inference, e.g. sentiment analysis, image classification, object detection, question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# feeding a sample text through pipeline\n",
    "result = classifier('We are very happy to show you the HuggingFace Transformers Library.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding a list of text through pipeline\n",
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "results = classifier(X_train)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a specific model in pipeline\n",
    "[AutoModels documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)  \n",
    "- for pytorch, `import AutoModelForSequenceClassification from transformers`\n",
    "- for tensorflow, `import TFAutoModelForSequenceClassification from transformers`  \n",
    "\n",
    "[AutoTokenizer documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier1 = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model and Tokenizer without pipeline\n",
    "Note: you'll get same results with or without pipeline!  \n",
    "Recommended to use pipeline unless you want to finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english' # 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "token_ids = tokenizer(\"We are very happy to show you the HuggingFace Transformers Library.\")\n",
    "print(token_ids)\n",
    "print('In input_ids, tokens 101 and 102 are the beginning and end of string tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [PreTrainedTokenizer documentation](https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) for parameter details.  \n",
    "Specifically for `return_tensors` parameter: click [here](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.return_tensors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"We are very happy to show you the HuggingFace Transformers Library.\", \"We hope you don't hate it.\"]\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()] # convert from label_id to class name\n",
    "\n",
    "print(outputs)\n",
    "print(predictions)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Reloading Saved Models and Tokenizers\n",
    "Convenient because you can save a tf model and load to use it in pytorch (and vice versa)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and tokenizer (esp. after finetuning)\n",
    "save_directory = 'saved'\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Model Hub](https://huggingface.co/models)\n",
    "Use any model uploaded by community  \n",
    "To use any model, copy the name in your chosen model (e.g. `oliverguhr/german-sentiment-bert`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'oliverguhr/german-sentiment-bert'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train_german = [\"Mit keinem guten Ergebnis\", \"Das war unfair\", # negative\n",
    "            \"nicht so schlecht wie erwartet\", \"Das war gut!\", # positive\n",
    "            \"Sie fahrt ein grunes Auto\"] # neutral\n",
    "\n",
    "# if don't have 'return_tensors' argument, it will return python list of integers instead of tensor\n",
    "batch = tokenizer(X_train_german, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "outputs = model(batch)\n",
    "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "labels = tf.argmax(predictions, axis=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in labels.numpy()]\n",
    "\n",
    "print(labels)\n",
    "print('Correct answer should be:')\n",
    "print('[negative, negative, positive, positive, neutral]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pipelines with non-default models (i.e. other models in Model Hub)\n",
    "- if the model in question fits the default pipelines, see [this](https://huggingface.co/docs/transformersquicktour#use-another-model-and-tokenizer-in-the-pipeline) for more details\n",
    "- if that model does not fit into any default pipeline, you'll need to use it without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'oliverguhr/german-sentiment-bert'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train_german = [\"Mit keinem guten Ergebnis\", \"Das war unfair\", # negative\n",
    "            \"nicht so schlecht wie erwartet\", \"Das war gut!\", # positive\n",
    "            \"Sie fahrt ein grunes Auto\"] # neutral\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "result = classifier(X_train_german)\n",
    "print(result)\n",
    "print('Correct answer should be:')\n",
    "print('negative, negative, positive, positive, neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "If you canâ€™t find a model for your use-case, you will need to fine-tune a pretrained model on your data.   \n",
    "\n",
    "In Tensorflow, models can be directly trained using Keras and `fit` method. See this [fine-tuning tutorial](https://huggingface.co/transformers/v4.8.2/training.html#fine-tuning-with-keras).  \n",
    "  \n",
    "For PyTorch, you can use the Transformers library class Trainer to fine-tune or train a model from scratch. See the [tutorial](https://huggingface.co/transformers/v4.8.2/training.html#fine-tuning-in-pytorch-with-the-trainer-api) for more details.  \n",
    "Alternatively, you can fine-tune it in native PyTorch. See this [tutorial](https://huggingface.co/transformers/v4.8.2/training.html#fine-tuning-in-native-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Implement the model J2s used for TIL NLP\n",
    "[`harshit345/xlsr-wav2vec-speech-emotion-recognition`](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)  \n",
    "Note: this model uses pytorch, so import the right packages from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # requirement packages\n",
    "# !pip install git+https://github.com/huggingface/datasets.git\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install torchaudio\n",
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"harshit345/xlsr-wav2vec-speech-emotion-recognition\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for a sample\n",
    "path = 'test.wav'   \n",
    "outputs = predict(path, sampling_rate)\n",
    "print(outputs) # correct label is angry"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6425117d13823fa8044a2c07c859b613f2362d94b282c9a4162ba20339fd2c4d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6425117d13823fa8044a2c07c859b613f2362d94b282c9a4162ba20339fd2c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
