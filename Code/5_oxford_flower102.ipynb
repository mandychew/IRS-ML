{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqeHwQ3w4xeY"
      },
      "source": [
        "# tf Pre-Trained Models\n",
        "Recommended to use: \n",
        "- efficientnet_v2 if dk what to use  \n",
        "\n",
        "Models to avoid: \n",
        "- vgg\n",
        "    - slow to train\n",
        "    - not accurate\n",
        "- mobilenet\n",
        "    - unless running on mobile phone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTIrnJQ15Zga"
      },
      "source": [
        "## Things to Note\n",
        "xOut = Dense(num_of_classes)(x)\n",
        "- e.g. this case we have 102 classes\n",
        "\n",
        "If accuracy is bad, can add more Dense layers in between Flatten and output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FDXfdIyR55yY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M16Af_AH7Ctm"
      },
      "source": [
        "## Preprocessing\n",
        "note: some models, such as ResNet, includes a preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # FYI, to organise images that are not categorised in folders:\n",
        "# import os, shutil\n",
        "# import pandas as pd\n",
        "\n",
        "# data = pd.read_csv('train.csv', index_col='id')\n",
        "\n",
        "# Path = ''\n",
        "# for index in data.index:\n",
        "#   if not os.path.exists(os.path.join(Path, data['target'][index])):\n",
        "#     os.mkdir(path=os.path.join(Path, data['target'][index]))\n",
        "#   shutil.copy(os.path.join(Path, data['Image'][index]), os.path.join(Path, data['target'][index]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "for i, v in dict.items():\n",
        "    os.makedirs(v, exist_ok=True)\n",
        "    shutil.move('train/' +  i, v+'/'+i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF-RrID8Anwr",
        "outputId": "8ec39342-cd7a-46db-a98f-63a4fe57d987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5132 files belonging to 102 classes.\n",
            "Found 1357 files belonging to 102 classes.\n",
            "Found 1700 files belonging to 102 classes.\n"
          ]
        }
      ],
      "source": [
        "train = tf.keras.preprocessing.image_dataset_from_directory( # latest tensorflow version uses util\n",
        "    directory='../Datasets/Oxford Flower split/train',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical', # labels are one hot encoded\n",
        "    image_size=(224, 224), # ideally same as original data\n",
        "    batch_size=32, # must be greater or equal to batch_size in model.fit()\n",
        "    shuffle=True # for train and val, leave as True\n",
        "    )\n",
        "\n",
        "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory='../Datasets/Oxford Flower split/val',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        "    )\n",
        "\n",
        "test = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory='../Datasets/Oxford Flower split/test',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = train.cache().prefetch(buffer_size = tf.data.AUTOTUNE) # default buffer_size = AUTOTUNE\n",
        "val = val.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "test = test.cache().prefetch(buffer_size = tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzuvfH7P7ASm"
      },
      "source": [
        "## Build the Model with pre-trained model\n",
        "### EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjORu7pg-BxY",
        "outputId": "0b6298d8-a5bd-4bda-a3c3-a061d416b9e9"
      },
      "outputs": [],
      "source": [
        "# with pre-trained model\n",
        "xIn = Input((224, 224, 3)) # 3rd input is 3 because some models need rgb\n",
        "net = tf.keras.applications.efficientnet.EfficientNetB0(weights='imagenet', include_top=False)\n",
        "x = net(xIn)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='swish')(x)\n",
        "xOut = Dense(102, activation='softmax')(x)\n",
        "\n",
        "model = Model(xIn, xOut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "EvIlhXKj78xG",
        "outputId": "c5c98484-6071-4f3d-831d-d6db32ea050d"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint('./best_model', monitor='val_accuracy', save_best_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "]\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(train, batch_size=4, validation_data=val, epochs=epochs, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices()\n",
        "#tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "num_epochs = range(1, epochs + 1)\n",
        "\n",
        "plt.plot(num_epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(num_epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(num_epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(num_epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSddB3zw0Qa8"
      },
      "outputs": [],
      "source": [
        "# with pre-trained model\n",
        "xIn = Input((224, 224, 3)) # 3rd input is 3 because some models need rgb\n",
        "x = tf.keras.applications.resnet50.preprocess_input(xIn) # some newer models don't include preprocessing\n",
        "resnet_model = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet') # this model is not a layer\n",
        "x = resnet_model(xIn)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='swish')(x)\n",
        "xOut = Dense(102, activation='softmax')(x)\n",
        "\n",
        "model_res = Model(xIn, xOut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_res.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
        "model_res.summary()\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "]\n",
        "epochs = 10\n",
        "\n",
        "history_res = model_res.fit(train, batch_size=8, validation_data=val, epochs=epochs, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW0AD7Sd36J3"
      },
      "outputs": [],
      "source": [
        "acc = history_res.history['acc']\n",
        "val_acc = history_res.history['val_acc']\n",
        "loss = history_res.history['loss']\n",
        "val_loss = history_res.history['val_loss']\n",
        "\n",
        "num_epochs = range(1, epochs + 1)\n",
        "\n",
        "plt.plot(num_epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(num_epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(num_epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(num_epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using ImageDataGenerator and EfficientNet\n",
        "aka Data Augmentation  \n",
        "note: usually only augment training data, don't augment test and validation data\n",
        "\n",
        "! Don't blindly apply options in ImageDataGenerator\n",
        "- see how test data differs from training data, and choose based on that\n",
        "- if data augmentation is too strong, will harm accuracy instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        brightness_range=(-0.2, 0.2),\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True, \n",
        "        vertical_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'C:/Users/mandy/OneDrive/2022 IRS/ML/Datasets/Oxford Flower split/train',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory='C:/Users/mandy/OneDrive/2022 IRS/ML/Datasets/Oxford Flower split/val',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with pre-trained model\n",
        "xIn = Input((224, 224, 3)) # 3rd input is 3 because some models need rgb\n",
        "net = tf.keras.applications.efficientnet.EfficientNetB0(weights='imagenet', include_top=False)\n",
        "x = net(xIn)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='swish')(x)\n",
        "xOut = Dense(102, activation='softmax')(x)\n",
        "\n",
        "model_gen = Model(xIn, xOut)\n",
        "model_gen.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
        "model_gen.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "]\n",
        "\n",
        "epochs = 10\n",
        "history_gen = model_gen.fit(train_generator, steps_per_epoch=5132//32 + 1, epochs=epochs, callbacks=callbacks) \n",
        "# when using data generator, don't run val data through model.fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_gen.evaluate(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc = history_gen.history['acc']\n",
        "val_acc = history_gen.history['val_acc']\n",
        "loss = history_gen.history['loss']\n",
        "val_loss = history_gen.history['val_loss']\n",
        "\n",
        "num_epochs = range(1, 7)\n",
        "\n",
        "plt.plot(num_epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(num_epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "# plt.figure()\n",
        "\n",
        "plt.plot(num_epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(num_epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('data_generator_oxford_flowers.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare models"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "5_oxford_flower102.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
